"""
Script that can be used to automatically download data into Google Drive from satellite imagery
"""

import ee
from ee import batch
ee.Initialize()

modis = ee.ImageCollection("MOIS/006/MOD13Q1I")
gridmet = ee.ImageCollection("IDAHO_EPSCOR/GRIDMET")
noah = ee.ImageCollection("NASA/FLDAS/NOAH01/C/GL/M/V001")


#def merge_imagecollection(ImageCollection):
#	mergeBands = lambda image, previous: ee.Image(previous).addBands(image)
#
#	merged_image = ImageCollection.iterate(mergeBands, ee.Image([]));
#
#	return merged_image

def monthly_mapping(n, start_date, imageColl):
	"""Define monthly mapping over image collection

	"""

	# offset from start_date
	init = start_date.advance(n, "month")

	# advance 1 week
	end = init.advance(1, "month")

	#filter and reduce

	return imageColl.filterDate(init, end).mean().set("time", init)

def process(start_date, end_date, datatype, feature):
	"""extract and process climatic data for year

	Input:

		start_date:
		end_date:
		datatype:
		feature:

	Output:


	"""

	if datatype == "modis":
		data = modis
	elif datatype == "gridmet":
		data = gridmet
	elif datatype == "noah":
		data = noah
	else:
		raise Exception("ImageCollection not defined. Check whether input is modis, gridmet or noah")

	# convert dates to ee data types
	start_date = ee.Date(start_date)
	end_date = ee.Date(end_date)
	
	# select feature
	data = data.select(feature).filterDate(start_date, end_date)
	
	data_reduced = data.filterDate(start_date, start_date).mean()

	# load regions: counties from a public fusion table, removing non-conus states
	# by using a custom filter
	nonCONUS = [2,15,60,66,69,72,78]
	counties = ee.FeatureCollection('ft:1ZMnPbFshUI3qbk9XE0H7t1N5CjsEGyl8lZfWfVn4')\
			.filter(ee.Filter.inList('STATEFP',nonCONUS).Not())

	# get mean values by county polygon
	features = data_reduced.reduceRegions(\
	collection = counties,\
	reducer = ee.Reducer.mean(),\
	scale = 4000
	)

	# add a new column for year to each feature in the features
	features = features.map(
		lambda feature: feature.set('time',start_date)
	)
	# Export ---------------------------------------------------------------------
	out = batch.Export.table.toDrive(
	collection = features,\
	description = '{}_{}'.format(feature, start_date),\
	folder = feature,\
	fileFormat = 'CSV',
	selectors = "mean, date, name"
	)
	
	# send batch to process as csv in server
	batch.Task.start(out)
	print("Process sent to cloud")


def main():

	for year in range(2008, 2009):
		print("Extracting data for year {}".format(year))
		# reduce daily precipitation data to annual total
		# load data: 365 images per year.
		# use IDAHO_EPSCOR/GRIDMET for temperature/rainfall etc...
		# MODIS/006/MOD13Q1 for EVI or NDVI
		# NASA/FLDAS/NOAH01/C/GL/M/V001 for evaporation - 198
		start_date  = '{}-04-01'.format(year)
		end_date = '{}-11-30'.format(year)
		feature = "NDVI"

		band_data = ee.ImageCollection("MODIS/006/MOD13Q1").select(feature)\
							.filterDate(start_date, end_date)

		# process weekly average
		start_date  = ee.Date(start_date)
		end_date = ee.Date(end_date)
		num_months = ee.Number(end_date.difference(start_date, "month")).round()

		mapping = lambda n: monthly_mapping(n, start_date, band_data)

		# map over each week
		data = ee.ImageCollection(
		ee.List.sequence(0, num_months).map(mapping)
		)

		# reduce the image collection to one image by summing the 365 daily rasters
		data_reduced = data.reduce(ee.Reducer.mean());

		# Image stats by regions: A spatial reducer -------------------------------------
		# Get mean annual precip by county

		# load regions: counties from a public fusion table, removing non-conus states
		# by using a custom filter
		nonCONUS = [2,15,60,66,69,72,78]
		counties = ee.FeatureCollection('ft:1ZMnPbFshUI3qbk9XE0H7t1N5CjsEGyl8lZfWfVn4')\
				.filter(ee.Filter.inList('STATEFP',nonCONUS).Not())

		# get mean values by county polygon
		features = data_reduced.reduceRegions(\
		collection = counties,\
		reducer = ee.Reducer.mean(),\
		scale = 4000
		)

		# add a new column for year to each feature in the features
		features = features.map(
			lambda feature: feature.set('Year',year)
		)
		# Export ---------------------------------------------------------------------
		out = batch.Export.table.toDrive(
		collection = features,\
		description = '{}_{}'.format(feature, year),\
		folder = feature,\
		fileFormat = 'CSV',
		selectors = "mean, date, name"
		)
		
		# send batch to process as csv in server
		batch.Task.start(out)
		print("Process sent to cloud")


if __name__ == "__main__":
	main()	
